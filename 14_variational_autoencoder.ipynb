{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE\n",
    "\n",
    "#### 変分近似\n",
    "Unfortunately, an an analytical solution for the posterior $p({\\bf{w}}|\\mathcal{D})$ in neural networks in unstractable. We therefore have to approximate the true posterior with a variational distribution $q({\\bf{w}}|{\\bf{\\theta}})$ of known functional form whose parameters we want to estimate. This can be done by minimizing the Kullback-Leibler divergence between $q({\\bf{w}}|{\\bf{\\theta}})$  and the true posterior $p({\\bf{w}}|\\mathcal{D})$. As in \\cite{blei2017variational}, the corresponding optimization objective or cost function is \n",
    "\n",
    "$\n",
    "    \\mathcal{J}(\\mathcal{D}, \\theta) = KL(q({\\bf{w}}|\\theta)|| p({\\bf{w}})) - \\mathcal{E}_{q({\\bf{w}}|\\theta)} \\log p(\\mathcal{D}|{\\bf{w}})\n",
    "$\n",
    "\n",
    "- $\\log p(x)$対数尤度を最大化したい。データが発生する分布$p(x)$が大きいほど実際にデータに則している\n",
    "- 真の事後分布$p(z|x)$は未知。近似$q(z|x)$を仮定\n",
    "- 対数尤度を分解：$q(z|x)$と$p(z|x)$測るKL-divergence項と事後分布$q(z|x)$からみた対数尤度$p(x|z)$のcross entropyに分解できる\n",
    "- 下限を大きくすれば、KL-divergence項が小さくなる（非負）\n",
    "\n",
    "参考資料\n",
    "https://i101330.hatenablog.com/entry/2016/11/12/140402\n",
    "\n",
    "#### VAE\n",
    "\n",
    "- 微分可能な推論inference$q(z|x)$＋生成(generative)モデル$p(x|z)$\n",
    "- Encoder: inference$q(z|x)$\n",
    "- Decoder: sampling$p(x|z)$\n",
    "- zはガウス分布を従うと仮定している場合、KL-divergenceを解析的に求められる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
